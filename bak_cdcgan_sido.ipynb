{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import (\n",
    "#     multiply,\n",
    "#     concatenate,\n",
    "# )\n",
    "# from tensorflow.keras.layers import GaussianNoise, GaussianDropout\n",
    "# from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import cv2\n",
    "\n",
    "# import PIL\n",
    "\n",
    "# from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py\n",
    "# from keras.datasets import mnist\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    ZeroPadding2D,\n",
    "    LeakyReLU,\n",
    "    UpSampling2D,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import pathlib\n",
    "\n",
    "import time\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  1.15.2\n",
      "Keras version:  2.2.4-tf\n",
      "Is eager execution enabled:  True\n",
      "Is there a GPU available:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: \", tf.VERSION)  # 1.15.2\n",
    "print(\"Keras version: \", tf.keras.__version__)  # 2.2.4-tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "print(\"Is eager execution enabled: \", tf.executing_eagerly())\n",
    "print(\"Is there a GPU available: \", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_full_tfrecord = \"/data/fp85.tfrecord\"\n",
    "dir_save = \"rooms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@moritzkrger/speeding-up-keras-with-tfrecord-datasets-5464f9836c36\n",
    "\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    # Create a description of the features.\n",
    "    feature_description = {\n",
    "        \"floorplan\": tf.io.FixedLenFeature(\n",
    "            [28, 28, 6], tf.float32, default_value=tf.zeros([28, 28, 6], tf.float32)\n",
    "        ),\n",
    "        \"plan_id\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"norm_year\": tf.io.FixedLenFeature([], tf.float32, default_value=-1.0),\n",
    "        \"sido\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "        \"norm_area\": tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n",
    "        \"num_rooms\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "        \"num_baths\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "        \"brands\": tf.io.FixedLenFeature(\n",
    "            [12], tf.int64, default_value=tf.zeros([12], tf.int64)\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    return (parsed_example[\"floorplan\"], parsed_example[\"num_rooms\"])\n",
    "\n",
    "\n",
    "def _onehot_fp(fp, *rest):\n",
    "    \"\"\"\n",
    "    Replace unit layer with outdoor layer and restrict function space layers indoor\n",
    "    \"\"\"\n",
    "    fp_func = fp[:, :, 1:6] * tf.reshape(fp[:, :, 0], (28, 28, 1))\n",
    "    fp_out = tf.reshape(1 - fp[:, :, 0], (28, 28, 1))\n",
    "    return (tf.concat([fp_out, fp_func], axis=2), *rest)\n",
    "\n",
    "\n",
    "def _rescale_fp(fp, *rest):\n",
    "    \"\"\"\n",
    "    Rescale from [0, 1] to [-1, 1]\n",
    "    \"\"\"\n",
    "    return (fp * 2 - 1, *rest)\n",
    "\n",
    "\n",
    "def _onehot_sido(_, sido, *rest):\n",
    "    \"\"\"\n",
    "    Make sido label one-hot array\n",
    "    \"\"\"\n",
    "    n_classes = 9\n",
    "    return (_, tf.one_hot(sido, n_classes), *rest)\n",
    "\n",
    "\n",
    "def _onehot_bath(_, n_rms, *rest):\n",
    "    \"\"\"\n",
    "    Make sido label one-hot array\n",
    "    \"\"\"\n",
    "    n_classes = 2\n",
    "    return (_, tf.one_hot(n_rms - 1, n_classes), *rest)\n",
    "\n",
    "\n",
    "def _visualize_fp_onehot(fps):\n",
    "    # adjusted for different luminance\n",
    "    channel_to_rgba = np.array(\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, 0.0],  # ignore outdoor mask\n",
    "            [0.0, 0.33, 0.0, 0.0],  # entrance to green L30\n",
    "            [1.0, 0.25, 0.0, 0.0],  # LDK to red L57\n",
    "            [0.0, 0.26, 1.0, 0.0],  # bedroom to blue L40\n",
    "            [0.83, 0.87, 0.0, 0.0],  # balcony to yellow L85\n",
    "            [0.0, 0.81, 0.76, 0.0],\n",
    "        ]\n",
    "    )  # bathroom to cyan L75\n",
    "\n",
    "    # make colors subtractive\n",
    "    channel_to_rgba[1:6, 0:3] -= 1\n",
    "\n",
    "    # put it on white\n",
    "    fps_rgba = np.clip(\n",
    "        np.array([1.0, 1.0, 1.0, 1.0]) + (np.array(fps) @ channel_to_rgba), 0, 1\n",
    "    )\n",
    "    return fps_rgba\n",
    "\n",
    "\n",
    "def create_dataset(filepath, batch_size=8):\n",
    "\n",
    "    # This works with arrays as well\n",
    "    dataset = tf.data.TFRecordDataset(filepath, compression_type=\"GZIP\")\n",
    "\n",
    "    # Maps the parser on every filepath in the array. You can set the number of parallel loaders here\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=4)\n",
    "\n",
    "    # make the floorplan one-hot-ish\n",
    "    dataset = dataset.map(_onehot_fp, num_parallel_calls=4)\n",
    "\n",
    "    # rescale to [-1, 1]\n",
    "    dataset = dataset.map(_rescale_fp, num_parallel_calls=4)\n",
    "\n",
    "    # sido one-hot\n",
    "    dataset = dataset.map(_onehot_bath, num_parallel_calls=4)\n",
    "\n",
    "    # This dataset will go on forever\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    # Set the number of datapoints you want to load and shuffle\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    # Set the batchsize\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = create_dataset(path_full_tfrecord)\n",
    "# dataset_iter = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "# plt.imshow(_visualize_fp_onehot(next(dataset_iter)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDCGAN\n",
    "\n",
    "참고:\n",
    "- https://github.com/gaborvecsei/CDCGAN-Keras/\n",
    "- https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDCGAN_fp_sido:\n",
    "    def __init__(self, dir_save=\"cdcgan\"):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 6\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        self.n_classes = 2\n",
    "\n",
    "        self.latent_dim = 100\n",
    "        self.dir_save = dir_save\n",
    "        self.model_name = \"cdcgan\"\n",
    "\n",
    "        pathlib.Path(self.dir_save).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(\n",
    "            loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        # Build and compile the combined model\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        self.combined = self.build_combined(self.generator, self.discriminator)\n",
    "        self.combined.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        # Prepare noise input\n",
    "        input_z = Input(shape=(self.latent_dim,))\n",
    "        dense_z_1 = Dense(1024)(input_z)\n",
    "        act_z_1 = Activation(\"tanh\")(dense_z_1)\n",
    "\n",
    "        dense_z_2 = Dense(128 * 7 * 7)(act_z_1)\n",
    "        bn_z_1 = BatchNormalization()(dense_z_2)\n",
    "        reshape_z = Reshape((7, 7, 128), input_shape=(128 * 7 * 7,))(bn_z_1)\n",
    "\n",
    "        # Prepare Conditional (label) input\n",
    "        input_c = Input(shape=(self.n_classes,))\n",
    "        dense_c_1 = Dense(1024)(input_c)\n",
    "        act_c_1 = Activation(\"tanh\")(dense_c_1)\n",
    "        dense_c_2 = Dense(128 * 7 * 7)(act_c_1)\n",
    "        bn_c_1 = BatchNormalization()(dense_c_2)\n",
    "        reshape_c = Reshape((7, 7, 128), input_shape=(128 * 7 * 7,))(bn_c_1)\n",
    "\n",
    "        # Combine input source\n",
    "        concat_z_c = Concatenate()([reshape_z, reshape_c])\n",
    "\n",
    "        # Image generation with the concatenated inputs\n",
    "        up_1 = UpSampling2D(size=(2, 2))(concat_z_c)\n",
    "        conv_1 = Conv2D(64, (5, 5), padding=\"same\")(up_1)\n",
    "        act_1 = Activation(\"tanh\")(conv_1)\n",
    "        up_2 = UpSampling2D(size=(2, 2))(act_1)\n",
    "        conv_2 = Conv2D(self.channels, (5, 5), padding=\"same\")(up_2)\n",
    "        act_2 = Activation(\"tanh\")(conv_2)\n",
    "\n",
    "        model = Model(inputs=[input_z, input_c], outputs=act_2, name=\"generator\",)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        # generated image input\n",
    "        input_gen_image = Input(shape=self.img_shape)\n",
    "        conv_1_image = Conv2D(64, (5, 5), padding=\"same\")(input_gen_image)\n",
    "        act_1_image = Activation(\"tanh\")(conv_1_image)\n",
    "        pool_1_image = MaxPooling2D(pool_size=(2, 2))(act_1_image)\n",
    "        conv_2_image = Conv2D(128, (5, 5))(pool_1_image)\n",
    "        act_2_image = Activation(\"tanh\")(conv_2_image)\n",
    "        pool_2_image = MaxPooling2D(pool_size=(2, 2))(act_2_image)\n",
    "\n",
    "        # label input\n",
    "        input_c = Input(shape=(self.n_classes,))\n",
    "        dense_1_c = Dense(1024)(input_c)\n",
    "        act_1_c = Activation(\"tanh\")(dense_1_c)\n",
    "        dense_2_c = Dense(5 * 5 * 128)(act_1_c)\n",
    "        bn_c = BatchNormalization()(dense_2_c)\n",
    "        reshaped_c = Reshape((5, 5, 128))(bn_c)\n",
    "\n",
    "        # combine input source\n",
    "        concat = Concatenate()([pool_2_image, reshaped_c])\n",
    "\n",
    "        # real or fake 0-1 output\n",
    "        flat = Flatten()(concat)\n",
    "        dense_1 = Dense(1024)(flat)\n",
    "        act_1 = Activation(\"tanh\")(dense_1)\n",
    "        dense_2 = Dense(1)(act_1)\n",
    "        act_2 = Activation(\"sigmoid\")(dense_2)\n",
    "\n",
    "        model = Model(\n",
    "            inputs=[input_gen_image, input_c], outputs=act_2, name=\"discriminator\",\n",
    "        )\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def build_combined(self, g, d):\n",
    "        # The generator takes noise and class as input and generates imgs\n",
    "        input_z = Input(shape=(self.latent_dim,))\n",
    "        input_c = Input(shape=(self.n_classes,))\n",
    "        gen_image = g([input_z, input_c])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        d.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images and classes as input and determines validity\n",
    "        valid = d([gen_image, input_c])\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        model = Model(inputs=[input_z, input_c], outputs=valid, name=\"combined\")\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        path_tfrecord,\n",
    "        epochs=201,\n",
    "        initial_epoch=0,\n",
    "        batch_size=128,\n",
    "        save_interval=1,\n",
    "    ):\n",
    "        if initial_epoch:\n",
    "            self.load_model(initial_epoch - 1)\n",
    "\n",
    "        n_examples = sum(\n",
    "            1 for _ in tf.data.TFRecordDataset(path_tfrecord, compression_type=\"GZIP\")\n",
    "        )\n",
    "        batches_per_epoch = math.ceil(n_examples / batch_size)\n",
    "\n",
    "        # create a floorplan data iterator with size of half batch\n",
    "        train_dataset = create_dataset(path_tfrecord, batch_size=batch_size // 2)\n",
    "        dataset_iter = tf.compat.v1.data.make_one_shot_iterator(train_dataset)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid_full = np.ones((batch_size, 1))\n",
    "        valid_half = np.ones((batch_size // 2, 1))\n",
    "        fake_half = np.zeros((batch_size // 2, 1))\n",
    "\n",
    "        # zero knowlegde guess for starting loss\n",
    "        d_loss_fake = [-math.log(0.5), 0.5]\n",
    "        g_loss = -math.log(0.5)\n",
    "\n",
    "        # start time\n",
    "\n",
    "        t0 = time.time()\n",
    "        t1 = t0\n",
    "\n",
    "        for epoch in range(initial_epoch, initial_epoch + epochs):\n",
    "            for _ in range(batches_per_epoch):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Load a half batch of floorplan images and labels\n",
    "                batch = next(dataset_iter)\n",
    "                img, label_half = batch[0].numpy(), batch[1].numpy()\n",
    "\n",
    "                # Sample noise and generate a half batch of new images\n",
    "                noise_half = np.random.normal(0, 1, (batch_size // 2, self.latent_dim))\n",
    "\n",
    "                gen_img = self.generator.predict([noise_half, label_half])\n",
    "\n",
    "                # Train the discriminator (real classified as ones and generated as zeros)\n",
    "                d_loss_real = self.discriminator.train_on_batch(\n",
    "                    [img, label_half], valid_half\n",
    "                )\n",
    "                d_loss_fake = self.discriminator.train_on_batch(\n",
    "                    [gen_img, label_half], fake_half\n",
    "                )\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Sample noise and generate a batch of new images\n",
    "                noise_full = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                label_full = np.tile(label_half, (2, 1))\n",
    "\n",
    "                # Train the generator (wants discriminator to mistake images as real)\n",
    "                g_loss = self.combined.train_on_batch(\n",
    "                    [noise_full, label_full], valid_full\n",
    "                )\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0 or epoch == initial_epoch + epochs - 1:\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\n",
    "                    \"%d [D loss: %f, D_fake acc.: %.1f%%] [G loss: %f]\"\n",
    "                    % (epoch, d_loss[0], 100 * d_loss_fake[1], g_loss),\n",
    "                    end=\" \",\n",
    "                )\n",
    "\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "                t2 = time.time()\n",
    "                print(f\"{t2-t1:.1f}s elapsed since, total {t2-t0:.1f}s\")\n",
    "                t1 = t2\n",
    "\n",
    "        # save model after finish\n",
    "        self.save_model(epoch)\n",
    "\n",
    "    def _visualize_fp(self, fps):\n",
    "        # adjusted for different luminance\n",
    "        channel_to_rgba = np.array(\n",
    "            [\n",
    "                [0.0, 0.0, 0.0, 1.0],  # unit mask to alpha\n",
    "                [0.0, 0.33, 0.0, 0.0],  # entrance to green L30\n",
    "                [1.0, 0.25, 0.0, 0.0],  # LDK to red L57\n",
    "                [0.0, 0.26, 1.0, 0.0],  # bedroom to blue L40\n",
    "                [0.83, 0.87, 0.0, 0.0],  # balcony to yellow L85\n",
    "                [0.0, 0.81, 0.76, 0.0],\n",
    "            ]\n",
    "        )  # bathroom to cyan L75\n",
    "\n",
    "        # make colors subtractive\n",
    "        channel_to_rgba[1:6, 0:3] -= 1\n",
    "\n",
    "        # put it on transparent white\n",
    "        fps_rgba = np.clip(\n",
    "            np.array([1.0, 1.0, 1.0, 0.0]) + (np.array(fps) @ channel_to_rgba), 0, 1\n",
    "        )\n",
    "        return fps_rgba\n",
    "\n",
    "    def _visualize_fp_onehot(self, fps):\n",
    "        # adjusted for different luminance\n",
    "        channel_to_rgba = np.array(\n",
    "            [\n",
    "                [0.0, 0.0, 0.0, 0.0],  # ignore outdoor mask\n",
    "                [0.0, 0.33, 0.0, 0.0],  # entrance to green L30\n",
    "                [1.0, 0.25, 0.0, 0.0],  # LDK to red L57\n",
    "                [0.0, 0.26, 1.0, 0.0],  # bedroom to blue L40\n",
    "                [0.83, 0.87, 0.0, 0.0],  # balcony to yellow L85\n",
    "                [0.0, 0.81, 0.76, 0.0],\n",
    "            ]\n",
    "        )  # bathroom to cyan L75\n",
    "\n",
    "        # make colors subtractive\n",
    "        channel_to_rgba[1:6, 0:3] -= 1\n",
    "\n",
    "        # put it on white\n",
    "        fps_rgba = np.clip(\n",
    "            np.array([1.0, 1.0, 1.0, 1.0]) + (np.array(fps) @ channel_to_rgba), 0, 1\n",
    "        )\n",
    "        return fps_rgba\n",
    "\n",
    "    def save_imgs(self, epoch, r=5, c=-1):\n",
    "        if c == -1:\n",
    "            c = self.n_classes\n",
    "        ### create a fixed noise\n",
    "        # save the current state\n",
    "        random_state = np.random.get_state()\n",
    "        # set same noise for images\n",
    "        np.random.seed(1106)\n",
    "        noise = np.repeat(np.random.normal(0, 1, (r, self.latent_dim)), c, axis=0)\n",
    "        # reset the state\n",
    "        np.random.set_state(random_state)\n",
    "\n",
    "        label = np.tile(np.eye(c), (r, 1))\n",
    "\n",
    "        gen_imgs = self.generator.predict([noise, label])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        # visualize as rgba\n",
    "        gen_imgs_rgba = self._visualize_fp_onehot(gen_imgs)\n",
    "\n",
    "        fig, axs = plt.subplots(r, c, figsize=(c / 2.5, r / 2.5), dpi=300)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs_rgba[cnt, :, :, :])\n",
    "                axs[i, j].axis(\"off\")\n",
    "                cnt += 1\n",
    "        fig.savefig(\n",
    "            f\"{self.dir_save}/{self.model_name}_{epoch:06}.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0,\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self, end_epoch):\n",
    "        for path, model in zip(\n",
    "            [\n",
    "                f\"{self.dir_save}/{self.model_name}_{end_epoch:06}_{name}.h5\"\n",
    "                for name in [\"gen\", \"disc\"]\n",
    "            ],\n",
    "            [self.generator, self.discriminator],\n",
    "        ):\n",
    "            with h5py.File(path, \"w\") as file:\n",
    "                weight = model.get_weights()\n",
    "                for i in range(len(weight)):\n",
    "                    file.create_dataset(\"weight\" + str(i), data=weight[i])\n",
    "\n",
    "    def load_model(self, end_epoch):\n",
    "        for path, model in zip(\n",
    "            [\n",
    "                f\"{self.dir_save}/{self.model_name}_{end_epoch:06}_{name}.h5\"\n",
    "                for name in [\"gen\", \"disc\"]\n",
    "            ],\n",
    "            [self.generator, self.discriminator],\n",
    "        ):\n",
    "            with h5py.File(path, \"r\") as file:\n",
    "                weight = []\n",
    "                for i in range(len(file.keys())):\n",
    "                    weight.append(file[\"weight\" + str(i)][:])\n",
    "\n",
    "            model.set_weights(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 28, 28, 6)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 28, 28, 64)   9664        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 28, 28, 64)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         3072        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 14, 14, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1024)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 10, 10, 128)  204928      max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3200)         3280000     activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10, 10, 128)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 3200)         12800       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 5, 5, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 5, 5, 128)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5, 5, 256)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 6400)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         6554624     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1024)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            1025        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,066,113\n",
      "Trainable params: 10,059,713\n",
      "Non-trainable params: 6,400\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         103424      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         3072        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1024)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6272)         6428800     activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 6272)         6428800     activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 6272)         25088       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 6272)         25088       dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 7, 7, 128)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 7, 7, 128)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7, 7, 256)    0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 14, 14, 256)  0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 14, 14, 64)   409664      up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 14, 14, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 28, 28, 64)   0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 6)    9606        up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 28, 28, 6)    0           conv2d_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 13,433,542\n",
      "Trainable params: 13,408,454\n",
      "Non-trainable params: 25,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"combined\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "generator (Model)               (None, 28, 28, 6)    13433542    input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "discriminator (Model)           (None, 1)            10066113    generator[1][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,499,655\n",
      "Trainable params: 13,408,454\n",
      "Non-trainable params: 10,091,201\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CDCGAN_fp_sido(dir_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "0 [D loss: 0.385995, D_fake acc.: 76.6%] [G loss: 2.730809] 27.4s elapsed since, total 27.4s\n",
      "10 [D loss: 0.002596, D_fake acc.: 100.0%] [G loss: 0.051477] 253.5s elapsed since, total 280.9s\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    path_full_tfrecord,\n",
    "    epochs=1000,\n",
    "    #     initial_epoch=1000,\n",
    "    batch_size=128,\n",
    "    save_interval=10,\n",
    ")\n",
    "\n",
    "# 128*1000 -> less than 202s (now skip training on overpowering one)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
