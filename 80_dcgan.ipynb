{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import (\n",
    "#     multiply,\n",
    "#     concatenate,\n",
    "# )\n",
    "# from tensorflow.keras.layers import MaxPooling2D\n",
    "# from tensorflow.keras.layers import GaussianNoise, GaussianDropout\n",
    "# from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import cv2\n",
    "\n",
    "# import PIL\n",
    "\n",
    "# from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py\n",
    "# from keras.datasets import mnist\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    ZeroPadding2D,\n",
    "    LeakyReLU,\n",
    "    UpSampling2D,\n",
    "    Conv2D,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import pathlib\n",
    "\n",
    "import time\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  1.15.2\n",
      "Keras version:  2.2.4-tf\n",
      "Is eager execution enabled:  True\n",
      "Is there a GPU available:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: \", tf.VERSION)  # 1.15.2\n",
    "print(\"Keras version: \", tf.keras.__version__)  # 2.2.4-tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "print(\"Is eager execution enabled: \", tf.executing_eagerly())\n",
    "print(\"Is there a GPU available: \", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_full_tfrecord = \"/data/fp.tfrecord\"\n",
    "dir_save = \"dcgan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@moritzkrger/speeding-up-keras-with-tfrecord-datasets-5464f9836c36\n",
    "\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    # Create a description of the features.\n",
    "    feature_description = {\n",
    "        \"floorplan\": tf.io.FixedLenFeature(\n",
    "            [28, 28, 6], tf.float32, default_value=tf.zeros([28, 28, 6], tf.float32)\n",
    "        ),\n",
    "        \"plan_id\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"norm_year\": tf.io.FixedLenFeature([], tf.float32, default_value=-1.0),\n",
    "        \"sido\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "        \"norm_area\": tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n",
    "        \"num_rooms\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "        \"num_baths\": tf.io.FixedLenFeature([], tf.int64, default_value=-1),\n",
    "        \"brands\": tf.io.FixedLenFeature(\n",
    "            [12], tf.int64, default_value=tf.zeros([12], tf.int64)\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    return (parsed_example[\"floorplan\"],)\n",
    "    # return (parsed_example[\"floorplan\"], parsed_example[\"brands\"])\n",
    "\n",
    "\n",
    "def _onehot_fp(fp, *rest):\n",
    "    \"\"\"\n",
    "    Replace unit layer with outdoor layer and restrict function space layers indoor\n",
    "    \"\"\"\n",
    "    fp_func = fp[:, :, 1:6] * tf.reshape(fp[:, :, 0], (28, 28, 1))\n",
    "    fp_out = tf.reshape(1 - fp[:, :, 0], (28, 28, 1))\n",
    "    return (tf.concat([fp_out, fp_func], axis=2), *rest)\n",
    "\n",
    "\n",
    "def _rescale_fp(fp, *rest):\n",
    "    \"\"\"\n",
    "    Rescale from [0, 1] to [-1, 1]\n",
    "    \"\"\"\n",
    "    return (fp * 2 - 1, *rest)\n",
    "\n",
    "\n",
    "def _rescale_fp_brand(fp, brand, *rest):\n",
    "    \"\"\"\n",
    "    Rescale from [0, 1] to [-1, 1]\n",
    "    \"\"\"\n",
    "    return (fp * 2 - 1, brand * 2 - 1, *rest)\n",
    "\n",
    "def _visualize_fp_onehot(fps):\n",
    "    # adjusted for different luminance\n",
    "    channel_to_rgba = np.array(\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, 0.0],  # ignore outdoor mask\n",
    "            [0.0, 0.33, 0.0, 0.0],  # entrance to green L30\n",
    "            [1.0, 0.25, 0.0, 0.0],  # LDK to red L57\n",
    "            [0.0, 0.26, 1.0, 0.0],  # bedroom to blue L40\n",
    "            [0.83, 0.87, 0.0, 0.0],  # balcony to yellow L85\n",
    "            [0.0, 0.81, 0.76, 0.0],\n",
    "        ]\n",
    "    )  # bathroom to cyan L75\n",
    "\n",
    "    # make colors subtractive\n",
    "    channel_to_rgba[1:6, 0:3] -= 1\n",
    "\n",
    "    # put it on white\n",
    "    fps_rgba = np.clip(\n",
    "        np.array([1.0, 1.0, 1.0, 1.0]) + (np.array(fps) @ channel_to_rgba), 0, 1\n",
    "    )\n",
    "    return fps_rgba\n",
    "\n",
    "\n",
    "def create_dataset(filepath, batch_size=8):\n",
    "\n",
    "    # This works with arrays as well\n",
    "    dataset = tf.data.TFRecordDataset(filepath, compression_type=\"GZIP\")\n",
    "\n",
    "    # Maps the parser on every filepath in the array. You can set the number of parallel loaders here\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=4)\n",
    "\n",
    "    # make the floorplan one-hot-ish\n",
    "    dataset = dataset.map(_onehot_fp, num_parallel_calls=4)\n",
    "\n",
    "    # rescale to [-1, 1]\n",
    "    dataset = dataset.map(_rescale_fp, num_parallel_calls=4)\n",
    "\n",
    "    # This dataset will go on forever\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    # Set the number of datapoints you want to load and shuffle\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    # Set the batchsize\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = create_dataset(path_full_tfrecord)\n",
    "# dataset_iter = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "# plt.imshow(_visualize_fp_onehot(next(dataset_iter)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN_fp:\n",
    "    def __init__(self, dir_save=\"dcgan\"):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 6\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.dir_save = dir_save\n",
    "\n",
    "        pathlib.Path(self.dir_save).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(\n",
    "            loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid, name=\"combined\")\n",
    "        self.combined.summary()\n",
    "        self.combined.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential(name=\"generator\")\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential(name=\"discriminator\")\n",
    "\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"\n",
    "            )\n",
    "        )\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(\n",
    "        self, path_tfrecord, epochs=201, initial_epoch=0, batch_size=128, save_interval=1,\n",
    "    ):\n",
    "        if initial_epoch:\n",
    "            self.load_model(initial_epoch - 1)\n",
    "\n",
    "        n_examples = sum(\n",
    "            1 for _ in tf.data.TFRecordDataset(path_tfrecord, compression_type=\"GZIP\")\n",
    "        )\n",
    "        batches_per_epoch = math.ceil(n_examples / batch_size)\n",
    "\n",
    "        # create a floorplan data iterator with size of half batch\n",
    "        train_dataset = create_dataset(path_tfrecord, batch_size=batch_size // 2)\n",
    "        dataset_iter = tf.compat.v1.data.make_one_shot_iterator(train_dataset)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid_full = np.ones((batch_size, 1))\n",
    "        valid_half = np.ones((batch_size // 2, 1))\n",
    "        fake_half = np.zeros((batch_size // 2, 1))\n",
    "\n",
    "        # zero knowlegde guess for starting loss\n",
    "        d_loss_fake = [-math.log(0.5), 0.5]\n",
    "        g_loss = -math.log(0.5)\n",
    "\n",
    "        # start time\n",
    "\n",
    "        t0 = time.time()\n",
    "        t1 = t0\n",
    "\n",
    "        for epoch in range(initial_epoch, initial_epoch + epochs):\n",
    "            for _ in range(batches_per_epoch):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Load a half batch of floorplan images\n",
    "                imgs = next(dataset_iter)[0].numpy()\n",
    "\n",
    "                # Sample noise and generate a half batch of new images\n",
    "                noise = np.random.normal(0, 1, (batch_size // 2, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator (real classified as ones and generated as zeros)\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid_half)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake_half)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Sample noise and generate a batch of new images\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Train the generator (wants discriminator to mistake images as real)\n",
    "                g_loss = self.combined.train_on_batch(noise, valid_full)\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0 or epoch == initial_epoch + epochs - 1:\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\n",
    "                    \"%d [D loss: %f, acc.: %.1f%%] [G loss: %f]\"\n",
    "                    % (epoch, d_loss[0], 100 * d_loss[1], g_loss),\n",
    "                    end=\" \",\n",
    "                )\n",
    "\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "                t2 = time.time()\n",
    "                print(f\"{t2-t1:.1f}s elapsed since, total {t2-t0:.1f}s\")\n",
    "                t1 = t2\n",
    "\n",
    "        # save model after finish\n",
    "        self.save_model(epoch)\n",
    "\n",
    "    def _visualize_fp(self, fps):\n",
    "        # adjusted for different luminance\n",
    "        channel_to_rgba = np.array(\n",
    "            [\n",
    "                [0.0, 0.0, 0.0, 1.0],  # unit mask to alpha\n",
    "                [0.0, 0.33, 0.0, 0.0],  # entrance to green L30\n",
    "                [1.0, 0.25, 0.0, 0.0],  # LDK to red L57\n",
    "                [0.0, 0.26, 1.0, 0.0],  # bedroom to blue L40\n",
    "                [0.83, 0.87, 0.0, 0.0],  # balcony to yellow L85\n",
    "                [0.0, 0.81, 0.76, 0.0],\n",
    "            ]\n",
    "        )  # bathroom to cyan L75\n",
    "\n",
    "        # make colors subtractive\n",
    "        channel_to_rgba[1:6, 0:3] -= 1\n",
    "\n",
    "        # put it on transparent white\n",
    "        fps_rgba = np.clip(\n",
    "            np.array([1.0, 1.0, 1.0, 0.0]) + (np.array(fps) @ channel_to_rgba), 0, 1\n",
    "        )\n",
    "        return fps_rgba\n",
    "\n",
    "    def _visualize_fp_onehot(self, fps):\n",
    "        # adjusted for different luminance\n",
    "        channel_to_rgba = np.array(\n",
    "            [\n",
    "                [0.0, 0.0, 0.0, 0.0],  # ignore outdoor mask\n",
    "                [0.0, 0.33, 0.0, 0.0],  # entrance to green L30\n",
    "                [1.0, 0.25, 0.0, 0.0],  # LDK to red L57\n",
    "                [0.0, 0.26, 1.0, 0.0],  # bedroom to blue L40\n",
    "                [0.83, 0.87, 0.0, 0.0],  # balcony to yellow L85\n",
    "                [0.0, 0.81, 0.76, 0.0],\n",
    "            ]\n",
    "        )  # bathroom to cyan L75\n",
    "\n",
    "        # make colors subtractive\n",
    "        channel_to_rgba[1:6, 0:3] -= 1\n",
    "\n",
    "        # put it on white\n",
    "        fps_rgba = np.clip(\n",
    "            np.array([1.0, 1.0, 1.0, 1.0]) + (np.array(fps) @ channel_to_rgba), 0, 1\n",
    "        )\n",
    "        return fps_rgba\n",
    "\n",
    "    def save_imgs(self, epoch, r=5, c=5):\n",
    "        ### create a fixed noise\n",
    "        # save the current state\n",
    "        random_state = np.random.get_state()\n",
    "        # set same noise for images\n",
    "        np.random.seed(1106)\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        # reset the state\n",
    "        np.random.set_state(random_state)\n",
    "\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        # visualize as rgba\n",
    "        gen_imgs_rgba = self._visualize_fp_onehot(gen_imgs)\n",
    "        # gen_imgs_rgba = self._visualize_fp(gen_imgs)\n",
    "\n",
    "        fig, axs = plt.subplots(r, c, figsize=(2, 2), dpi=300)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs_rgba[cnt, :, :, :])\n",
    "                axs[i, j].axis(\"off\")\n",
    "                cnt += 1\n",
    "        fig.savefig(\n",
    "            f\"{self.dir_save}/dcgan_{epoch:06}.png\", bbox_inches=\"tight\", pad_inches=0\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self, end_epoch):\n",
    "        for path, model in zip(\n",
    "            [\n",
    "                f\"{self.dir_save}/dcgan_{end_epoch:06}_{name}.h5\"\n",
    "                for name in [\"gen\", \"disc\"]\n",
    "            ],\n",
    "            [self.generator, self.discriminator],\n",
    "        ):\n",
    "            with h5py.File(path, \"w\") as file:\n",
    "                weight = model.get_weights()\n",
    "                for i in range(len(weight)):\n",
    "                    file.create_dataset(\"weight\" + str(i), data=weight[i])\n",
    "\n",
    "    def load_model(self, end_epoch):\n",
    "        for path, model in zip(\n",
    "            [\n",
    "                f\"{self.dir_save}/dcgan_{end_epoch:06}_{name}.h5\"\n",
    "                for name in [\"gen\", \"disc\"]\n",
    "            ],\n",
    "            [self.generator, self.discriminator],\n",
    "        ):\n",
    "            with h5py.File(path, \"r\") as file:\n",
    "                weight = []\n",
    "                for i in range(len(file.keys())):\n",
    "                    weight.append(file[\"weight\" + str(i)][:])\n",
    "\n",
    "            model.set_weights(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 32)        1760      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 395,169\n",
      "Trainable params: 394,273\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 6)         3462      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 6)         0         \n",
      "=================================================================\n",
      "Total params: 859,078\n",
      "Trainable params: 858,694\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Model: \"combined\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 28, 28, 6)         859078    \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 1)                 395169    \n",
      "=================================================================\n",
      "Total params: 1,254,247\n",
      "Trainable params: 858,694\n",
      "Non-trainable params: 395,553\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN_fp(dir_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "0 [D loss: 0.781079, acc.: 50.8%] [G loss: 0.973619] 73.3s elapsed since, total 73.3s\n",
      "5 [D loss: 0.630773, acc.: 64.8%] [G loss: 1.009738] 355.1s elapsed since, total 428.4s\n",
      "10 [D loss: 0.610953, acc.: 63.3%] [G loss: 1.008298] 355.0s elapsed since, total 783.4s\n",
      "15 [D loss: 0.595425, acc.: 68.0%] [G loss: 1.120060] 355.8s elapsed since, total 1139.3s\n",
      "20 [D loss: 0.553890, acc.: 75.8%] [G loss: 1.544904] 356.0s elapsed since, total 1495.2s\n",
      "25 [D loss: 0.572437, acc.: 68.8%] [G loss: 1.537775] 355.8s elapsed since, total 1851.0s\n",
      "30 [D loss: 0.295971, acc.: 89.1%] [G loss: 1.996181] 356.4s elapsed since, total 2207.5s\n",
      "35 [D loss: 0.416275, acc.: 78.1%] [G loss: 2.085396] 356.1s elapsed since, total 2563.6s\n",
      "40 [D loss: 0.285836, acc.: 89.8%] [G loss: 2.372544] 355.0s elapsed since, total 2918.6s\n",
      "45 [D loss: 0.143209, acc.: 94.5%] [G loss: 4.151238] 355.3s elapsed since, total 3273.9s\n",
      "50 [D loss: 0.408986, acc.: 78.1%] [G loss: 3.901664] 355.1s elapsed since, total 3629.0s\n",
      "55 [D loss: 0.287100, acc.: 85.9%] [G loss: 3.818654] 356.1s elapsed since, total 3985.2s\n",
      "60 [D loss: 0.031500, acc.: 99.2%] [G loss: 3.902395] 356.1s elapsed since, total 4341.3s\n",
      "65 [D loss: 0.088929, acc.: 97.7%] [G loss: 4.844267] 355.5s elapsed since, total 4696.8s\n",
      "70 [D loss: 0.133052, acc.: 97.7%] [G loss: 4.050116] 356.2s elapsed since, total 5053.0s\n",
      "75 [D loss: 0.147872, acc.: 97.7%] [G loss: 3.669268] 356.3s elapsed since, total 5409.3s\n",
      "80 [D loss: 0.063850, acc.: 98.4%] [G loss: 5.327070] 355.9s elapsed since, total 5765.1s\n",
      "85 [D loss: 0.095401, acc.: 97.7%] [G loss: 2.785644] 356.6s elapsed since, total 6121.7s\n",
      "90 [D loss: 0.076209, acc.: 97.7%] [G loss: 6.083637] 355.5s elapsed since, total 6477.2s\n",
      "95 [D loss: 0.127225, acc.: 95.3%] [G loss: 5.419488] 356.0s elapsed since, total 6833.2s\n"
     ]
    }
   ],
   "source": [
    "dcgan.train(\n",
    "    path_full_tfrecord,\n",
    "    epochs=100,\n",
    "    # initial_epoch=,\n",
    "    batch_size=128,\n",
    "    save_interval=5,\n",
    ")\n",
    "\n",
    "# 128*1000 -> less than 202s (now skip training on overpowering one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
